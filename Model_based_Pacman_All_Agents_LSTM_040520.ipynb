{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_based_Pacman_All_Agents_LSTM_040520.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1rskkMC2YHwWw9eChUucJlriDHOKwhPHX",
      "authorship_tag": "ABX9TyPu3k18eQ5g5srUTn5v/3EV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/60_Days_RL_Challenge/blob/master/Model_based_Pacman_All_Agents_LSTM_040520.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4jLAGgyjflE",
        "colab_type": "text"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hdnkq3rcJNF",
        "colab_type": "text"
      },
      "source": [
        "Used to save Pacman video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp0jzlXIcFfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fss3NXoMcIjw",
        "colab_type": "code",
        "outputId": "9e28418d-008c-4994-e708-40f9d99f38c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!apt-get install ffmpeg"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (46.1.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 111 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "e56ece65-ef28-4e0c-cff0-75e00d6b45e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-4yxeos97\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-4yxeos97\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=5f943d9bdf92617389dbc8a022996ddaa37699b38d625d328ba249cc6f05c7de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2ubo6xeg/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-skqu3_qx\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-skqu3_qx\n",
            "Collecting gym<0.16.0,>=0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/01/8771e8f914a627022296dab694092a11a7d417b6c8364f0a44a8debca734/gym-0.15.7.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.38.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines, gym\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=4edf11862df8cc4c92a4ab050dfb7a612ec495c1e30e8393a9cc6655ab56e40f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v2hkqgwc/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-cp36-none-any.whl size=1648840 sha256=a6c9e72a6abdde4969215ecb6b2140326ffb9a0be680946b9cba083fa280b016\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/60/6a/f9c27ae133abaf5a5687ed2fa8ed19627d7fac5d843a27572b\n",
            "Successfully built baselines gym\n",
            "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gym, baselines\n",
            "  Found existing installation: gym 0.17.1\n",
            "    Uninstalling gym-0.17.1:\n",
            "      Successfully uninstalled gym-0.17.1\n",
            "Successfully installed baselines-0.1.6 gym-0.15.7\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.5MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.8MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/1a/0df85d2bddbca33665d2148173d3281b290ac054b5f50163ea735740ac7b/GitPython-3.1.1-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 58.8MB/s \n",
            "\u001b[?25hCollecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.4.5.1)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.9)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/52/ca35448b56c53a079d3ffe18b1978c6e424f6d4df02404877094c89f5bfb/gitdb-4.0.4-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/27/b1/e379cfb7c07bbf8faee29c4a1a2469dbea525f047c2b454c4afdefa20a30/smmap-3.0.2-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, watchdog, gql, pathtools, graphql-core\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=3e9ac95bfcdcfc602f1014b361c54da118ae8033977ecbb55ea4472de1e84d5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=d375dc11e980a4ce505dea51258559243ba481ab6eb82c846dcb56a306bf8473\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=3bd4b5e7d7bf1b0de3920ef071c7335ee180ea4c453a6db36ea3a356fbaeaf97\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=f4ba8e28713b7b0e1419e748c27b5dbaf9dae01afe63217948d8712921fed5e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=bf7e498d0320f273b0d4ac2723bc9c9bb40775a541a0640fb4cabdb90fd9333d\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built subprocess32 watchdog gql pathtools graphql-core\n",
            "Installing collected packages: subprocess32, pathtools, watchdog, docker-pycreds, configparser, sentry-sdk, shortuuid, graphql-core, gql, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.1 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.4 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.2 subprocess32-3.5.4 wandb-0.8.35 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI287_Yjxn3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from benchmark.envs import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from benchmark.wrapper import AtariARIWrapper\n",
        "\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from custom.EarlyStopping import EarlyStopping_loss\n",
        "from custom.model import *\n",
        "from custom.LSTM_model import *\n",
        "from custom.custom_wrappers import DeterminsticNoopResetEnv \n",
        "# Imported required for the Model-based RL\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "from baselines.common.atari_wrappers import EpisodicLifeEnv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "1fa88897-a566-42c5-8aeb-d1ee23a46ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login ################"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRn5uXDhgmQs",
        "colab_type": "text"
      },
      "source": [
        "# General Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2y__x5ysYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class General_functions():\n",
        "    def __init__(self, ENV_NAME,n_actions,possible_positions):\n",
        "        self.ENV_NAME = ENV_NAME\n",
        "        self.env = gym.make(self.ENV_NAME)\n",
        "        self.env = EpisodicLifeEnv(DeterminsticNoopResetEnv(self.env,66))        \n",
        "        self.env = AtariARIWrapper(self.env)\n",
        "        self.initial_info_labels = self.env.labels()\n",
        "        self.key_list = ['enemy_blinky_x', 'enemy_blinky_y','enemy_pinky_x','enemy_pinky_y','enemy_inky_x','enemy_inky_y', 'enemy_sue_x', 'enemy_sue_y', 'player_x', 'player_y']\n",
        "        \n",
        "    \n",
        "        self.prev_action_counter =  False\n",
        "        self.repeated_end = False # Nawid -  Sets self.repeated end as false initially\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.n_actions = n_actions\n",
        "        # possible positions is numpy array with most of the possible positions that the agent can go to\n",
        "        self.possible_positions = possible_positions\n",
        "        self.possible_positions_list = self.possible_positions.tolist()\n",
        "\n",
        "    def one_hot(self,i):\n",
        "        a = np.zeros(self.n_actions, 'uint8')\n",
        "        a[i-1] = 1\n",
        "        return a\n",
        "\n",
        "    def state_conversion(self,obs,info_labels):\n",
        "        state = [info_labels[word] for word in self.key_list if word in info_labels]                \n",
        "        state = np.array(state)\n",
        "        return state\n",
        "    \n",
        "    def next_position(self,state, action):\n",
        "        next_position = state[-2:].copy()\n",
        "        #print(next_position)\n",
        "        if action == 1:\n",
        "            next_position[1] = next_position[1] - 2 \n",
        "        elif action == 2:\n",
        "            next_position[0] = next_position[0] + 2\n",
        "        elif action == 3:\n",
        "            next_position[0] = next_position[0] - 2\n",
        "        elif action == 4:\n",
        "            next_position[1] = next_position[1] + 2\n",
        "        \n",
        "        if next_position.tolist() in self.possible_positions_list: # possible positions will be a list which is fed into the network \n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "            \n",
        "    ''' This is a general case which can be used with both the controller and the data collector\n",
        "    def random_action_selection(self, state,prev_action = None):\n",
        "        while True:\n",
        "            action = np.random.randint(1,5) \n",
        "            feasible_action = self.next_position(state,action)\n",
        "            if feasible_action:\n",
        "                self.prev_action_counter = True\n",
        "                #print('action:',action)\n",
        "                return action\n",
        "            else:\n",
        "                if self.prev_action_counter and prev_action !=None:\n",
        "                    #print('repeated action:',prev_action)\n",
        "                    return prev_action\n",
        "    '''\n",
        "\n",
        "    \n",
        "    def check_all_agents(self,info_label,next_info_label):\n",
        "        repeated = np.equal(info_label,next_info_label).all()\n",
        "        if repeated:\n",
        "            self.repeated_end= True\n",
        "\n",
        "    def check_state(self,state, next_state):\n",
        "        #print(state[2:])\n",
        "        repeated_state = np.equal(state[-2:], next_state[-2:]).all()\n",
        "        if repeated_state:\n",
        "            self.prev_action_counter = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJgI07xT7eo",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWTGqLduK7au",
        "colab_type": "code",
        "outputId": "a881fc5e-4ee4-43d5-c26e-0ad8360852a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "class Data_collection(General_functions):\n",
        "    def __init__(self,ENV_NAME,n_actions, possible_positions):\n",
        "        super(Data_collection,self).__init__(ENV_NAME,n_actions,possible_positions)\n",
        "        self.dataset_random = [] # Where the data is saved each time\n",
        "\n",
        "    def collate_data(self,random_dataset, rl_dataset):\n",
        "        rand_data = np.array(random_dataset)\n",
        "        num_rand_examples = len(rand_data)\n",
        "        D_train = rand_data[:int(-num_rand_examples*1/5)] \n",
        "        D_valid = rand_data[int(-num_rand_examples*1/5):]\n",
        "        print(\"number random examples:\",num_rand_examples, 'len(D_train_rand)', len(D_train),'len(D_valid_rand)', len(D_valid))\n",
        "        if len(rl_dataset) > 0:\n",
        "            # Adds the rl dataset to the random dataset if there is any present\n",
        "            rl_data = np.array(rl_dataset)\n",
        "            num_rl_examples = len(rl_data)\n",
        "            D_rl_train = rl_data[:int(-num_rl_examples*1/5)] \n",
        "            D_rl_valid = rl_data[int(-num_rl_examples*1/5):]\n",
        "                        \n",
        "            D_train = np.concatenate([D_train, D_rl_train], axis = 0)\n",
        "            D_valid = np.concatenate([D_valid, D_rl_valid], axis = 0)\n",
        "            print(\"number rl examples:\",num_rl_examples, 'len(D_rl_train)', len(D_rl_train),'len(D_valid_rand)', len(D_rl_valid))\n",
        "            \n",
        "        #print(\"len(D_train):\", len(D_train), 'len(D_valid)', len(D_valid))\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        \n",
        "        sff = np.arange(len(D_train))\n",
        "        np.random.shuffle(sff)\n",
        "        D_train = D_train[sff]\n",
        "        \n",
        "        #print('D_train shape',D_train.shape)\n",
        "\n",
        "        # Create the input and output for the train\n",
        "        X_train_obs = np.array([obs for obs,_,_,_,_ in D_train]) # Takes obs and action\n",
        "        X_train_obs = X_train_obs.astype(np.float32) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        X_train_act = np.array([act for _,_,_,_,act in D_train])\n",
        "        \n",
        "        # Env output\n",
        "        y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "        #print('y train:',y_env_train[0:100,:])\n",
        "        #print('X train:',X_train_obs[0:100,:])\n",
        "        y_env_train = y_env_train.astype(np.float32) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        \n",
        "\n",
        "        y_env_train = y_env_train - X_train_obs \n",
        "        \n",
        "        # Next state output\n",
        "        X_val_obs = np.array([obs for obs,_,_,_,_ in D_valid]) # Takes obs and action\n",
        "        X_val_obs = X_val_obs.astype(np.float32) # Need to change it to a int16 so it is signed ( so negative values can be calculated)        \n",
        "        X_val_act = np.array([act for _,_,_,_,act in D_valid])\n",
        "\n",
        "        y_env_val = np.array([no for _,no,_,_,_ in D_valid])\n",
        "        y_env_val = y_env_val.astype(np.float32)\n",
        "        y_env_val = y_env_val - X_val_obs \n",
        "\n",
        "        env_train_data, env_val_data = (X_train_obs, X_train_act, y_env_train), (X_val_obs, X_val_act, y_env_val)\n",
        "        return env_train_data, env_val_data \n",
        "\n",
        "    def setup_dataset(self,env_train_data, env_val_data):\n",
        "        # Unpack data\n",
        "        (X_env_train_obs, X_env_train_act, y_env_train), (X_env_val_obs, X_env_val_act, y_env_val) = env_train_data, env_val_data\n",
        "    \n",
        "        # Concatentates the normalised states with the one hot vector for the actions\n",
        "        X_env_train = np.concatenate((X_env_train_obs,X_env_train_act),axis=1)\n",
        "        X_env_val = np.concatenate((X_env_val_obs,X_env_val_act),axis=1)\n",
        "\n",
        "        # Pack data tuples\n",
        "        env_train_data, env_val_data = (X_env_train, y_env_train),(X_env_val, y_env_val) \n",
        "        return env_train_data, env_val_data\n",
        "    \n",
        "    def random_action_selection(self, state,prev_action = None):\n",
        "        while True:\n",
        "            action = np.random.randint(1,5) \n",
        "            feasible_action = self.next_position(state,action)\n",
        "            if feasible_action:\n",
        "                self.prev_action_counter = True\n",
        "                #print('action:',action)\n",
        "                return action, None\n",
        "            else:\n",
        "                infeasible_action_one_hot = self.one_hot(action)\n",
        "                #self.dataset_random.append([state, state, 0, False, infeasible_action_one_hot]) # NEED TO CHANGE WHERE THE DATA IS SAVED WHEN I AM USING THE DONES SINCE THE DONES ARE ADDED IN THE MAIN FUNCTION TO PAST VALUES   \n",
        "                if self.prev_action_counter and prev_action !=None:\n",
        "                    #print('repeated action:',prev_action)\n",
        "                    return prev_action, infeasible_action_one_hot\n",
        "    \n",
        "    def gather_random_trajectories(self,num_traj):\n",
        "        #dataset_random = []\n",
        "        \n",
        "        for n in range(num_traj):\n",
        "            if n % 10 ==0:\n",
        "                print('trajectory number:',n)\n",
        "                # Initial set up\n",
        "            #self.env.seed(0)\n",
        "            self.env = gym.make(self.ENV_NAME)\n",
        "            self.env = EpisodicLifeEnv(DeterminsticNoopResetEnv(self.env,noop_max=66))\n",
        "            self.env = AtariARIWrapper(self.env)\n",
        "            obs = self.env.reset()\n",
        "            \n",
        "            self.repeated_end = False\n",
        "            info_labels = self.env.labels() # Nawid - Used to get the current state\n",
        "            state = self.state_conversion(obs, info_labels) # Used to get the initial state\n",
        "            prev_action = None # Initialise prev action has having no action\n",
        "            \n",
        "            while True:\n",
        "                sampled_action, infeasible_action_one_hot = self.random_action_selection(state,prev_action)\n",
        "                #sampled_action = np.random.randint(1,5)\n",
        "                 \n",
        "                sampled_action_one_hot = self.one_hot(sampled_action)\n",
        "                next_obs, reward, done, next_info = self.env.step(sampled_action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                \n",
        "                next_state = self.state_conversion(next_obs, next_info_labels)\n",
        "                self.check_state(state,next_state)\n",
        "                self.check_all_agents(info_labels, next_info_labels) # need to use the info labels to predict the state as the info labels have all the informaiton\n",
        "                    \n",
        "                if not self.repeated_end:\n",
        "                    \n",
        "                    if infeasible_action_one_hot is not None:\n",
        "                        #print(infeasible_action_one_hot)\n",
        "                        fake_next_state = np.zeros_like(state) #  Need to instantiate a new version each time to prevent updating a single variable which will affect all places(eg lists) where the variable is added\n",
        "                        fake_next_state[0:-2] = next_state[0:-2].copy() # the enemy position of the fake next state is the current enemy position\n",
        "                        fake_next_state[-2:] = state[-2:].copy() # The agent position for the fake next state is the state before any action was taken\n",
        "\n",
        "                        self.dataset_random.append([state, fake_next_state, 0, False, infeasible_action_one_hot])\n",
        "\n",
        "                    self.dataset_random.append([state, next_state, reward, done,sampled_action_one_hot])\n",
        "                else:\n",
        "                    self.dataset_random[-1][-2] = True # sets the previous value to done\n",
        "                    done = True   \n",
        "\n",
        "                state = next_state.copy()\n",
        "                info_labels = next_info_labels.copy()\n",
        "                prev_action = sampled_action\n",
        "                if done:\n",
        "                    break    \n",
        "\n",
        "        return self.dataset_random    \n",
        "\n",
        "ENV_NAME = 'MsPacman-ramDeterministic-v4'\n",
        "n_actions = 4 #9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "\n",
        "possible_positions = np.load('/content/drive/My Drive/MsPacman-data/possible_pacman_positions.npy',allow_pickle=True)\n",
        "data_object = Data_collection(ENV_NAME,n_actions,possible_positions)\n",
        "data1 = data_object.gather_random_trajectories(10)\n",
        "\n",
        "rl_dataset = []\n",
        "env_train, env_val = data_object.collate_data(data1,rl_dataset)\n",
        "full_env_train, full_env_val = data_object.setup_dataset(env_train,env_val)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trajectory number: 0\n",
            "number random examples: 1574 len(D_train_rand) 1260 len(D_valid_rand) 314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dChd_BVNUKHR",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9wLqaPmqqm",
        "colab_type": "code",
        "outputId": "a39d75bb-70b9-4722-f899-a15f846429af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# General\n",
        "n_actions = 4\n",
        "# Time and date information\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}_{}-{}_{}\".format(now.day,now.month,now.year, now.hour, now.minute)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# Data related\n",
        "possible_positions = np.load('/content/drive/My Drive/MsPacman-data/possible_pacman_positions.npy',allow_pickle=True)\n",
        "\n",
        "# Making the network deterministic - https://pytorch.org/docs/stable/notes/randomness.html\n",
        "random_seed = 0\n",
        "if random_seed:    \n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    #np.random.seed(0)\n",
        "\n",
        "compulsory_dict = {'ENV_NAME': 'MsPacman-ramDeterministic-v4',\n",
        "                   'num_rand_trajectories': 3000,\n",
        "                   'device':'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "                   'pretrain_model': False,\n",
        "                   'num_RNN_layers': 1,\n",
        "                   'num_directions': 1,\n",
        "                   'input_dim': 2 + n_actions,\n",
        "                   'timesteps': 5,\n",
        "                   'BATCH_SIZE':128,\n",
        "                   'hidden_dim':1024,\n",
        "                   'output_dim': 2,\n",
        "                   'lr': 1e-4,\n",
        "                   'clip_grad':0.1,\n",
        "                   'AGGR_ITER':10,\n",
        "                   'load_data': False,\n",
        "                   'collect_data':False,\n",
        "                   'TRAIN_MODEL_ITER':1000,\n",
        "                   'STEPS_PER_AGGR':1000,\n",
        "                   'HORIZON_LENGTH':1,\n",
        "                   'NUM_ACTIONS_SEQUENCES':1,\n",
        "                   'random_seed':random_seed}\n",
        "\n",
        "wandb.init(entity=\"nerdk312\", project=\"ATARI_LABELS_MPC\",config = compulsory_dict)\n",
        "\n",
        "config = wandb.config\n",
        "if compulsory_dict['load_data']:\n",
        "    loaded_trajectories = '/content/drive/My Drive/MsPacman-data/rand_traj_3000-3_5_2020-11_23.npy'\n",
        "    config.loaded_trajectories = loaded_trajectories\n",
        "\n",
        "if compulsory_dict['pretrain_model']:\n",
        "    pretrained_model_dir = '/content/drive/My Drive/MsPacman-data/Env model/Env_model_21_4_2020-8_37.pt'\n",
        "    config.pretrained_model_dir = pretrained_model_dir"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/4yrjestc\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/4yrjestc</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIBf1pknbf_",
        "colab_type": "text"
      },
      "source": [
        "# Main - Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk1xCx6x7NZ2",
        "colab_type": "text"
      },
      "source": [
        "Data collection and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVutEvgUib8y",
        "colab_type": "code",
        "outputId": "fb970d3f-03a5-4d6a-9db9-53111c44c1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "data_collector = Data_collection(compulsory_dict['ENV_NAME'],n_actions,possible_positions)\n",
        "if compulsory_dict['load_data']:\n",
        "    rand_dataset = np.load(loaded_trajectories,allow_pickle=True)\n",
        "else:\n",
        "    rand_dataset = data_collector.gather_random_trajectories(NUM_RAND_TRAJECTORIES)\n",
        "    filename = '/content/drive/My Drive/MsPacman-data/rand_traj_{}-'.format(NUM_RAND_TRAJECTORIES) + date_time\n",
        "    np.save(filename,rand_dataset)\n",
        "\n",
        "empty_dataset = []\n",
        "env_train_data, env_val_data = data_collector.collate_data(rand_dataset,empty_dataset)\n",
        "norm_env_train_data, norm_env_val_data = data_collector.setup_dataset(env_train_data, env_val_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number random examples: 292700 len(D_train_rand) 234160 len(D_valid_rand) 58540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QG3_91bq-Y9I"
      },
      "source": [
        "Dynamics model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h62P5907GbX",
        "colab_type": "code",
        "outputId": "7fb0afb3-05c0-4cb8-a2fb-5e589eb0beac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "#env_model = NNDynamicsModel(n_actions + 2, 2).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "env_model = recurrent_dynamics_model(compulsory_dict['input_dim'], compulsory_dict['hidden_dim'], compulsory_dict['output_dim']).to(device)\n",
        "if compulsory_dict['pretrain_model']:\n",
        "    #env_model.load_state_dict(torch.load(pretrained_env_model))\n",
        "    checkpoint = torch.load(pretrained_env_model)\n",
        "    env_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    for parameter in env_model.parameters():\n",
        "        parameter.requires_grad = False\n",
        "    env_model.eval()\n",
        "else:\n",
        "    env_optimizer = torch.optim.Adam(env_model.parameters(),compulsory_dict['lr']) # Nawid - Optimizer for the env model\n",
        "    wandb.watch(env_model, log=\"all\")\n",
        "    env_model_name = 'Env_model'+ '_' + date_time\n",
        "    early_stopping_env = EarlyStopping_loss(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "    for n_iter in range(compulsory_dict['AGGR_ITER']):\n",
        "        if early_stopping_env.early_stop:\n",
        "            print('Early stopping')\n",
        "            break \n",
        "        train_recurrent_model(norm_env_train_data, norm_env_val_data, env_model, compulsory_dict['BATCH_SIZE'],compulsory_dict['timesteps'], compulsory_dict['TRAIN_MODEL_ITER'],env_optimizer,device,early_stopping_env)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 1/1000 [01:12<20:07:08, 72.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased/improved for Env_model_4_5_2020-10_30  (100000000000.000000 --> 2.926285).  Saving model ...\n",
            "epoch: 0 completed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0ada72c9cd6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Early stopping'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_recurrent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_env_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_env_val_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompulsory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BATCH_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompulsory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timesteps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompulsory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRAIN_MODEL_ITER'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/Unsupervised_state_representation/atariari/custom/LSTM_model.py\u001b[0m in \u001b[0;36mtrain_recurrent_model\u001b[0;34m(train_data, val_data, model, batch_size, timesteps, max_model_iter, optimizer, device, early_stopper, num_layers, num_directions, hidden_dim)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Recurrent model training loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTOm9XOX7kOL",
        "colab_type": "text"
      },
      "source": [
        "Training the reward function and MPC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F288CgeN6kmd",
        "colab_type": "text"
      },
      "source": [
        "# Old Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFoJKH5p3haq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_trajectories = '/content/gdrive/My Drive/MsPacman-data/rand_traj_3000-29_4_2020-9_8.npy'\n",
        "loaded_dataset = np.load(loaded_trajectories,allow_pickle=True)\n",
        "\n",
        "loaded_trajectory_2 = '/content/gdrive/My Drive/MsPacman-data/rand_traj_3000-29_4_2020-9_50.npy'\n",
        "loaded_dataset2 = np.load(loaded_trajectory_2,allow_pickle=True)\n",
        "\n",
        "collated_dataset = np.concatenate((loaded_dataset,loaded_dataset2),axis=0)\n",
        "filename = '/content/gdrive/My Drive/MsPacman-data/only_agent_position_6000_traj__prev_action_fix+failed_action_fix'\n",
        "np.save(filename,collated_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37-sB-PjPfRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def state_conversion(self,obs,info_labels):\n",
        "        state = []          \n",
        "        for key in info_labels:\n",
        "            if key in self.key_list: # Nawid - Only the first 14 info are crucial I believe\n",
        "                    state.append(info_labels[key])\n",
        "                    \n",
        "        state = np.array(state)\n",
        "        return state\n",
        "\n",
        "\n",
        "ENV_NAME = 'MsPacman-ramDeterministic-v4'\n",
        "env = gym.make(ENV_NAME)\n",
        "env = AtariARIWrapper(env)\n",
        "obs = env.reset()\n",
        "info_labels = env.labels()\n",
        "print(info_labels)\n",
        "state = []\n",
        "key_list = ['enemy_blinky_x', 'enemy_blinky_y','enemy_pinky_x','enemy_pinky_y', 'player_x', 'player_y']\n",
        "for key in info_labels:\n",
        "    if key in key_list:\n",
        "        state.append(info_labels[key])\n",
        "        print(key)\n",
        "\n",
        "state = [info_labels[word] for word in key_list if word in info_labels ]\n",
        "state = np.array(state)\n",
        "print(state.shape)\n",
        "for word in key_list:\n",
        "    if word in info_labels:\n",
        "        print(word)\n",
        "        print(info_labels[word])\n",
        "print(state)\n",
        "print('agent state',state[:-3])\n",
        "'''\n",
        "new_l = []\n",
        "for i in l:\n",
        "    if i['term'] in words:\n",
        "        new_l.append(i)\n",
        "#print([i for i in info_labels if i[\"term\"] in info_labels])\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}