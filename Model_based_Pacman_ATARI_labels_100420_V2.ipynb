{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_based_Pacman_ATARI_labels_100420_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN2Fgj2M0hRS4MelrL/uS2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/60_Days_RL_Challenge/blob/master/Model_based_Pacman_ATARI_labels_100420_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57XVXKX0pUf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4jLAGgyjflE",
        "colab_type": "text"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD40Izgp1JQ",
        "colab_type": "code",
        "outputId": "e3b3ca19-5b17-4270-f85e-c79365cc71ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "!pip install wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/mila-iqia/atari-representation-learning.git\n",
            "  Cloning git://github.com/mila-iqia/atari-representation-learning.git to /tmp/pip-req-build-w6bmq51o\n",
            "  Running command git clone -q git://github.com/mila-iqia/atari-representation-learning.git /tmp/pip-req-build-w6bmq51o\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from atariari==0.0.1) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->atariari==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->atariari==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: atariari\n",
            "  Building wheel for atariari (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atariari: filename=atariari-0.0.1-cp36-none-any.whl size=46584 sha256=95f8e9b0993dba7a76bf1497d5c177b5f36d76163306680c1e749e90820c9947\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-phkhfo0n/wheels/3d/69/51/5e436e5ae566c5b4dec5c53e65396d516459877a42a11d7aa4\n",
            "Successfully built atariari\n",
            "Installing collected packages: atariari\n",
            "Successfully installed atariari-0.0.1\n",
            "Collecting git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
            "  Cloning git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail to /tmp/pip-req-build-nynl8rmi\n",
            "  Running command git clone -q git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail /tmp/pip-req-build-nynl8rmi\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from a2c-ppo-acktr==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->a2c-ppo-acktr==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->a2c-ppo-acktr==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->a2c-ppo-acktr==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: a2c-ppo-acktr\n",
            "  Building wheel for a2c-ppo-acktr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a2c-ppo-acktr: filename=a2c_ppo_acktr-0.0.1-cp36-none-any.whl size=18833 sha256=3dd64c3a6824623955670ba983eef686b34c395b05be0178dcaa0227d07c9fa3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bkkhf4s1/wheels/91/52/02/ec5c530fd76d56a66934ee91abbdae5240b766be1dc176deb7\n",
            "Successfully built a2c-ppo-acktr\n",
            "Installing collected packages: a2c-ppo-acktr\n",
            "Successfully installed a2c-ppo-acktr-0.0.1\n",
            "Collecting git+git://github.com/openai/baselines\n",
            "  Cloning git://github.com/openai/baselines to /tmp/pip-req-build-3u4pibnn\n",
            "  Running command git clone -q git://github.com/openai/baselines /tmp/pip-req-build-3u4pibnn\n",
            "Collecting gym<0.16.0,>=0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/01/8771e8f914a627022296dab694092a11a7d417b6c8364f0a44a8debca734/gym-0.15.7.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.38.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines, gym\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp36-none-any.whl size=220664 sha256=93bf0e5268070fea43284220747c46eaf25d1e756deeec5d12071b04c850ce15\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lh4g4pdo/wheels/42/1c/91/28314e0cd1d2cc57cf8dd18b20c4c9a0f39ae518adc13caf24\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-cp36-none-any.whl size=1648840 sha256=cdfdc3fa6500d7868d2446e6fceecfacc0bf31ba4b4dd542fa69c81a92c517f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/60/6a/f9c27ae133abaf5a5687ed2fa8ed19627d7fac5d843a27572b\n",
            "Successfully built baselines gym\n",
            "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gym, baselines\n",
            "  Found existing installation: gym 0.17.1\n",
            "    Uninstalling gym-0.17.1:\n",
            "      Successfully uninstalled gym-0.17.1\n",
            "Successfully installed baselines-0.1.6 gym-0.15.7\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/18/ef5215832f523c29f6e0c19a5b87e0dd90fe40fb48ba38362f961be14e4f/wandb-0.8.31-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.6MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.1)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 18.2MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/1a/0df85d2bddbca33665d2148173d3281b290ac054b5f50163ea735740ac7b/GitPython-3.1.1-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 21.8MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/52/ca35448b56c53a079d3ffe18b1978c6e424f6d4df02404877094c89f5bfb/gitdb-4.0.4-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/27/b1/e379cfb7c07bbf8faee29c4a1a2469dbea525f047c2b454c4afdefa20a30/smmap-3.0.2-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, watchdog, gql, pathtools, graphql-core\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=11dfc8ede849d3cf65d6a96e6024a350e868162b4e48ff3189ef1613fec02fd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=6243a7683660a1febc5b518d52db190df2d324fa48b26ec787fb2d1ec8f612d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=4fa36cddfe584349c622f7e9bdc918a137739a11f3a32ca4c5b79ced9dbcea87\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=6f7e1be1f76585c15822a163281e16e5ee8693264d9f696ec7f187a232b530b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=295d7cc677818afeebdc8b1c805ba69833d03eacc85c040e3b3a874f88c66781\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built subprocess32 watchdog gql pathtools graphql-core\n",
            "Installing collected packages: subprocess32, pathtools, watchdog, shortuuid, graphql-core, gql, sentry-sdk, docker-pycreds, smmap, gitdb, GitPython, configparser, wandb\n",
            "Successfully installed GitPython-3.1.1 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.4 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.2 subprocess32-3.5.4 wandb-0.8.31 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZ9gMrxp5vU",
        "colab_type": "code",
        "outputId": "baa14471-926d-4b6c-b2bd-2f48935008f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI287_Yjxn3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "from __future__ import print_function\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Unsupervised_state_representation/atariari')\n",
        "\n",
        "import wandb\n",
        "\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, BatchSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from atariari.benchmark.envs import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import gym\n",
        "from atariari.benchmark.wrapper import AtariARIWrapper\n",
        "\n",
        "#from benchmark import *\n",
        "#from methods import utils\n",
        "\n",
        "# Needed to create dataloaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Imported required for the Model-based RL\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPbRVOCqA3q",
        "colab_type": "code",
        "outputId": "363a2bd2-aa0e-46c8-880c-bd0ac57fcc40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login ################"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKVfcEFEz9-t",
        "colab_type": "text"
      },
      "source": [
        "# Model setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaBzmIUDnqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping_loss(object):\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, verbose=False, wandb=None, name=\"\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = 1e11 # Nawid - Set a very high initial best loss\n",
        "        self.name = name\n",
        "        self.wandb = wandb\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score >= self.best_score: # Nawid - Inverse signs to take into minimising loss instead of maximising accuracy\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping for {self.name} counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(f'{self.name} has stopped')\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f'Validation loss decreased/improved for {self.name}  ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "\n",
        "        save_dir = self.wandb.run.dir\n",
        "        torch.save(model.state_dict(), save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.wandb.save(save_dir + \"/\" + self.name + \".pt\")\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGPEetVzTtOb",
        "colab": {}
      },
      "source": [
        "class NNDynamicsModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the next state, given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, obs_output_dim):\n",
        "        super(NNDynamicsModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, obs_output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "class NNRewardModel(nn.Module):\n",
        "    '''\n",
        "    Model that predict the reward given the current state and action\n",
        "    '''\n",
        "    def __init__(self, input_dim, reward_output_dim):\n",
        "        super(NNRewardModel, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, reward_output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n",
        "\n",
        "def model_MSEloss(y_pred,y_truth, device):\n",
        "    '''\n",
        "    Compute the MSE (Mean Squared Error)\n",
        "    '''\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))\n",
        "\n",
        "def model_CEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the CEloss\n",
        "    '''\n",
        "    y_truth = torch.Tensor(np.array(y_truth)).to(device)\n",
        "    return F.cross_entropy(y_pred, y_truth)\n",
        "\n",
        "def model_BCEloss(y_pred,y_truth,device):\n",
        "    '''\n",
        "    Compute the BCE (Binary cross entropy)\n",
        "    param y_pred: y_pred is a 2 dimensional n x 1 data tensor\n",
        "    param y_truth: y_truth is a 2D numpy array which later gets converted into a tensor\n",
        "    '''\n",
        "    n,c = y_pred.size()\n",
        "    weights = np.zeros((n,c))\n",
        "    pos = y_truth[y_truth==1].sum()\n",
        "    neg = n - pos\n",
        "    #nx = y_truth.cpu().data.numpy()\n",
        "    index = np.where(y_truth == 1)[0]\n",
        "    weights[:] = 1 #1/neg\n",
        "    weights[index] = neg* 1/pos\n",
        "    weights = torch.Tensor(weights).to(device)\n",
        "    #print('y_pred',y_pred.size())\n",
        "    #print('weights',weights.size())\n",
        "\n",
        "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
        "    #print('y_truth',y_truth.size())\n",
        "    return F.binary_cross_entropy(y_pred.view(-1).float(), y_truth.view(-1),weights.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRn5uXDhgmQs",
        "colab_type": "text"
      },
      "source": [
        "# General Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2y__x5ysYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class General_functions():\n",
        "    def __init__(self, ENV_NAME, feature_size,normalise_values,n_actions,state_mode = 'ATARIARI',encoder=None):\n",
        "        self.state_mode = state_mode\n",
        "        self.env = AtariARIWrapper(gym.make(ENV_NAME))\n",
        "        self.initial_info_labels = self.env.labels()\n",
        "\n",
        "        self.feature_size = feature_size \n",
        "        self.normalise_values = normalise_values\n",
        "\n",
        "        self.repeated_initial = True # Nawid - set repeated initial as true initially\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.encoder= encoder\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def one_hot(self,i):\n",
        "        a = np.zeros(self.n_actions, 'uint8')\n",
        "        a[i] = 1\n",
        "        return a\n",
        "\n",
        "\n",
        "    def state_conversion(self,obs,info_labels):\n",
        "        if self.state_mode == 'ATARIARI':\n",
        "            state = []\n",
        "            i = 0 \n",
        "            for key in info_labels :\n",
        "                if i < self.feature_size: # Nawid - Only the first 14 info are crucial I believe\n",
        "                    state.append(info_labels[key])\n",
        "                    i +=1\n",
        "                else:\n",
        "                    state = np.array(state)\n",
        "                    return state\n",
        "    \n",
        "        elif self.state_mode == 'STDIM': # Encoding is the state\n",
        "            assert self.encoder is not None\n",
        "            with torch.no_grad():\n",
        "                self.encoder.eval()\n",
        "                state = self.encoder(obs.float().to(self.device) / 255)\n",
        "                return state.cpu().detach().numpy()\n",
        "        else: # Image observations are the state or the RAM labels\n",
        "            return obs\n",
        "    \n",
        "    def check_initial(self, next_info_labels):\n",
        "        # Checks if there has been a change from the initial info labels to show that the initial lag period is over \n",
        "        if self.initial_info_labels == next_info_labels:\n",
        "            pass\n",
        "        else:\n",
        "            self.repeated_initial = False\n",
        "\n",
        "    def reward_conversion(self,reward,done,info_labels,next_info_labels):\n",
        "        # checks whether there has been a change in lives- change done to 1, otherwise the done should be fine regardless\n",
        "        if not info_labels['num_lives'] == next_info_labels['num_lives']: # Nawid - If there is a change in the number of lives -  The change in the number of lives occurs after the mspacman is resurrected\n",
        "            done = True \n",
        "            self.repeated_initial = True\n",
        "            self.initial_info_labels  = next_info_labels # Sets the new initial labels.\n",
        "        return reward, done  \n",
        "\n",
        "    def normalise_datapoint(self,x,scaler = None):\n",
        "        if self.normalise_values and not scaler == None:\n",
        "            x = scaler.fit_transform(x)\n",
        "        return x\n",
        "    \n",
        "    def inverse_normalise_datapoint(self,x, scaler = None):\n",
        "        if self.normalise_values and not scaler == None:\n",
        "            x = scaler.inverse_transform(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJgI07xT7eo",
        "colab_type": "text"
      },
      "source": [
        "# Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT3xItE78OyC",
        "colab_type": "code",
        "outputId": "b2a51711-bf23-4252-e782-86d741a2342e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "class Data_collection(General_functions):\n",
        "    def __init__(self,ENV_NAME, feature_size,normalise_values,n_actions, state_model = 'ATARIARI', encoder= None):\n",
        "        super(Data_collection,self).__init__(ENV_NAME,feature_size,normalise_values,n_actions, state_model, encoder)\n",
        "        \n",
        "    def gather_random_trajectories(self,num_traj):\n",
        "        dataset_random = []\n",
        "        #Env name could either be the RAM case or the generic case\n",
        "        # set pacman_done as initially zero\n",
        "        pacman_done = 0\n",
        "        i = 0\n",
        "        for n in range(num_traj):\n",
        "            if n % 10 ==0:\n",
        "                print('trajectory number :',n)\n",
        "            # Initial set up\n",
        "            obs = self.env.reset()\n",
        "            self.env.seed(0)\n",
        "\n",
        "            self.repeated_initial = True # Nawid- Used to represent the initial state\n",
        "            info_labels = self.env.labels() # Nawid -  Used to get the current state\n",
        "            \n",
        "            while True:\n",
        "                # Choosing action and env step\n",
        "                \n",
        "                sampled_action = np.random.randint(0,n_actions)\n",
        "                sampled_action_one_hot = self.one_hot(sampled_action)\n",
        "                next_obs, reward, done, next_info = self.env.step(sampled_action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                # self.repeated initial is set to true at first and it is is turned to true in the reward_collection class when done is true\n",
        "                \n",
        "                self.check_initial(next_info_labels) # checks if the initial value and the current value is the same, if it is not then it changes the repeated_initial bool\n",
        "                if self.repeated_initial: # If the initial state is repeating\n",
        "                    pass\n",
        "                        \n",
        "                else:\n",
        "                    # New state achieved, so save data\n",
        "                    state = self.state_conversion(obs, info_labels)\n",
        "                    next_state = self.state_conversion(next_obs, next_info_labels)\n",
        "                    # looks if ep is done and sets the new initial info state and sets the repeated_initial bool back to true\n",
        "                    reward,pacman_done = self.reward_conversion(reward, done, info_labels, next_info_labels) \n",
        "                    dataset_random.append([state, next_state,reward,pacman_done,sampled_action_one_hot])\n",
        "                    \n",
        "                obs = next_obs\n",
        "                info_labels = next_info_labels\n",
        "\n",
        "                # ends the loop when its done or pacman_done in the single life setting\n",
        "                if done or (single_life and pacman_done):\n",
        "                    break \n",
        "        \n",
        "        return dataset_random\n",
        "\n",
        "    def collate_data(self,random_dataset, rl_dataset):\n",
        "        rand_data = np.array(random_dataset)\n",
        "        num_rand_examples = len(rand_data)\n",
        "        D_train = rand_data[:int(-num_rand_examples*1/5)] \n",
        "        D_valid = rand_data[int(-num_rand_examples*1/5):]\n",
        "        print(\"number random examples:\",num_rand_examples, 'len(D_train_rand)', len(D_train),'len(D_valid_rand)', len(D_valid))\n",
        "        if len(rl_dataset) > 0:\n",
        "            # Adds the rl dataset to the random dataset if there is any present\n",
        "            rl_data = np.array(rl_dataset)\n",
        "            num_rl_examples = len(rl_data)\n",
        "            D_rl_train = rl_data[:int(-num_rl_examples*1/5)] \n",
        "            D_rl_valid = rl_data[int(-num_rl_examples*1/5):]\n",
        "                        \n",
        "            D_train = np.concatenate([D_train, D_rl_train], axis = 0)\n",
        "            D_valid = np.concatenate([D_valid, D_rl_valid], axis = 0)\n",
        "            print(\"number rl examples:\",num_rl_examples, 'len(D_rl_train)', len(D_rl_train),'len(D_valid_rand)', len(D_rl_valid))\n",
        "            \n",
        "        #print(\"len(D_train):\", len(D_train), 'len(D_valid)', len(D_valid))\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        \n",
        "        sff = np.arange(len(D_train))\n",
        "        np.random.shuffle(sff)\n",
        "        D_train = D_train[sff]\n",
        "        #print('D_train shape',D_train.shape)\n",
        "\n",
        "\n",
        "        # Create the input and output for the train\n",
        "        X_train_obs = np.array([obs for obs,_,_,_,_ in D_train]) # Takes obs and action\n",
        "        X_train_obs = X_train_obs.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        X_train_act = np.array([act for _,_,_,_,act in D_train])\n",
        "        \n",
        "\n",
        "        # Env output\n",
        "        y_env_train = np.array([no for _,no,_,_,_ in D_train])\n",
        "        y_env_train = y_env_train.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)\n",
        "        y_env_train = y_env_train - X_train_obs \n",
        "        #y_env_train = y_env_train - np.array([obs for obs,_,_,_,_ in D_train]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        # Reward's output\n",
        "        y_rew_train = np.array([[D] for _,_,_,D,_ in D_train])\n",
        "    \n",
        "        # Next state output\n",
        "        X_val_obs = np.array([obs for obs,_,_,_,_ in D_valid]) # Takes obs and action\n",
        "        X_val_obs = X_val_obs.astype(np.int16) # Need to change it to a int16 so it is signed ( so negative values can be calculated)        \n",
        "        X_val_act = np.array([act for _,_,_,_,act in D_valid])\n",
        "\n",
        "        y_env_val = np.array([no for _,no,_,_,_ in D_valid])\n",
        "        y_env_val = y_env_val.astype(np.int16)\n",
        "        y_env_val = y_env_val - X_val_obs \n",
        "        #y_env_val = y_env_val - np.array([obs for obs,_,_,_,_ in D_valid]) # y(state) = s(t+1) - s(t)\n",
        "\n",
        "        # Reward output\n",
        "        y_rew_val = np.array([[D] for _,_,_,D,_ in D_valid])\n",
        "    \n",
        "        env_train_data, env_val_data = (X_train_obs, X_train_act, y_env_train), (X_val_obs, X_val_act, y_env_val)\n",
        "        rew_train_data, rew_val_data = (X_train_obs, X_train_act, y_rew_train), (X_val_obs, X_val_act, y_rew_val)\n",
        "\n",
        "        return env_train_data, env_val_data, rew_train_data, rew_val_data \n",
        "\n",
        "    def normalise(self,train_data, val_data,scaler = None): # Nawid - Used to normalise each dimension individually\n",
        "        if self.normalise_values:\n",
        "            if scaler is None:\n",
        "                scaler = StandardScaler()\n",
        "                train_data = scaler.fit_transform(train_data)\n",
        "            else: \n",
        "                train_data = scaler.transform(train_data)        \n",
        "            val_data = scaler.transform(val_data)\n",
        "    \n",
        "        # Normalises the values if normalise is set to true, otherwise it returns the unnormalised values\n",
        "        return train_data, val_data, scaler\n",
        "\n",
        "    def normalise_dataset(self,env_train_data, env_val_data, rew_train_data, rew_val_data,X_env_obs_scaler = None, y_env_scaler=None, X_rew_obs_scaler= None):\n",
        "        # Unpack data\n",
        "        (X_env_train_obs, X_env_train_act, y_env_train), (X_env_val_obs, X_env_val_act, y_env_val) = env_train_data, env_val_data\n",
        "        (X_rew_train_obs, X_rew_train_act, y_rew_train), (X_rew_val_obs, X_rew_val_act, y_rew_val) = rew_train_data, rew_val_data\n",
        "    \n",
        "        # Normalise training and validation data\n",
        "        X_env_train_obs,X_env_val_obs, X_env_obs_scaler =  self.normalise(X_env_train_obs, X_env_val_obs, X_env_obs_scaler)\n",
        "        y_env_train, y_env_val, y_env_scaler = self.normalise(y_env_train, y_env_val, y_env_scaler)\n",
        "        X_rew_train_obs, X_rew_val_obs, X_rew_obs_scaler = self.normalise(X_rew_train_obs, X_rew_val_obs, X_rew_obs_scaler)\n",
        "    \n",
        "        # Concatentates the normalised states with the one hot vector for the actions\n",
        "        X_env_train = np.concatenate((X_env_train_obs,X_env_train_act),axis=1)\n",
        "        X_env_val = np.concatenate((X_env_val_obs,X_env_val_act),axis=1)\n",
        "        X_rew_train = np.concatenate((X_rew_train_obs,X_rew_train_act),axis=1)\n",
        "        X_rew_val = np.concatenate((X_rew_val_obs,X_rew_val_act),axis=1)\n",
        "\n",
        "        # Pack data tuples\n",
        "        env_train_data, env_val_data = (X_env_train, y_env_train),(X_env_val, y_env_val) \n",
        "        rew_train_data, rew_val_data = (X_rew_train, y_rew_train),(X_rew_val, y_rew_val)\n",
        "\n",
        "        return env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_obs_scaler, y_env_scaler, X_rew_obs_scaler\n",
        "    \n",
        "'''\n",
        "data_object = Data_collection(ENV_NAME,feature_size,normalise_on,5)\n",
        "data = data_object.gather_random_trajectories(1)\n",
        "rl_dataset = []\n",
        "env_train, env_val, rew_train, rew_val = data_object.collate_data(data,rl_dataset)\n",
        "norm_env_train, norm_env_val, norm_rew_train, norm_rew_val, X_obs_scaler, y_env_scaler, X_rew_scaler = data_object.normalise_dataset(env_train, env_val, rew_train, rew_val)\n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trajectory number : 0\n",
            "number random examples: 111 len(D_train_rand) 89 len(D_valid_rand) 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jEdA4yE3GfE",
        "colab_type": "text"
      },
      "source": [
        "# Controller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJqKzdpT6vLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class multi_model_based_control(General_functions):\n",
        "    def __init__(self,ENV_NAME, feature_size,normalise_values,n_actions,norm_scalers,env_model,rew_model, num_sequences,horizon_length, state_model = 'ATARIARI', encoder= None):\n",
        "        super(multi_model_based_control,self).__init__(ENV_NAME,feature_size,normalise_values,n_actions, state_model, encoder)\n",
        "        \n",
        "        self.env_model = env_model\n",
        "        self.rew_model = rew_model\n",
        "        self.horizon_length = horizon_length\n",
        "        self.num_sequences = num_sequences        \n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        if self.normalise_values == False:\n",
        "            self.env_input_scaler = None\n",
        "            self.env_output_scaler = None\n",
        "            self.rew_input_scaler = None\n",
        "        else:\n",
        "            self.env_input_scaler, self.env_output_scaler, self.rew_input_scaler = norm_scalers\n",
        "        print('normalise_values :',self.normalise_values)\n",
        "\n",
        "    def random_sampling_shooting(self,real_obs):\n",
        "        '''\n",
        "        Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n",
        "        '''\n",
        "        best_reward = -1e9\n",
        "        best_next_action = []\n",
        "        m_obs = np.array([real_obs for _ in range(self.num_sequences)])\n",
        "\n",
        "        # array that contains the rewards for all the sequence\n",
        "        unroll_rewards = np.zeros((self.num_sequences, 1)) \n",
        "        first_sampled_actions = []\n",
        "\n",
        "        self.env_model.eval()\n",
        "        self.rew_model.eval()\n",
        "\n",
        "        # Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon length' times\n",
        "        ## i.e roll a given number of trajectories in a single batch (to increase speed)\n",
        "        for t in range(self.horizon_length):\n",
        "            # sample actions for each sequence\n",
        "            sampled_actions = np.array([np.random.randint(0,self.n_actions) for _ in range(self.num_sequences)])\n",
        "            sampled_actions_one_hot = np.array([self.one_hot(action) for action in sampled_actions])\n",
        "            if isinstance(self.env_model,NNDynamicsModel): # Nawid-  If the env model is a neural net\n",
        "                #print('using dynamics model')\n",
        "                print('env input scaler:', self.env_input_scaler)\n",
        "                m_obs_env_scaled = self.normalise_datapoint(m_obs,self.env_input_scaler) # The env_input scaler should be none when normalisation is off\n",
        "                print('m obs env_scaled',m_obs_env_scaled)\n",
        "                env_model_input = np.concatenate([m_obs_env_scaled, sampled_actions_one_hot], axis = 1)\n",
        "                # compute the next state for each sequence\n",
        "                pred_obs = self.env_model(torch.tensor(env_model_input).to(self.device))\n",
        "\n",
        "                # inverse scaler transformation\n",
        "                print('env output scaler', self.env_output_scaler)\n",
        "                pred_obs = self.inverse_normalise_datapoint(pred_obs.cpu().detach().numpy(),self.env_output_scaler)\n",
        "                print('pred obs', pred_obs)\n",
        "                # add previous observation\n",
        "                next_obs = pred_obs + m_obs\n",
        "            else:\n",
        "                #print('using oracle state')            \n",
        "                next_obs = self.env_model.predict_states(m_obs,sampled_actions) # Nawid - Able to obtain the next state directly rather than predicting a change in states\n",
        "\n",
        "            if isinstance(self.rew_model, NNRewardModel):\n",
        "                print('rew input_scaler', self.rew_input_scaler)\n",
        "                m_obs_rew_scaled = self.normalise_datapoint(m_obs, self.rew_input_scaler)\n",
        "                print('m obs rew scaled', m_obs_rew_scaled)\n",
        "                rew_model_input = np.concatenate([m_obs_rew_scaled, sampled_actions_one_hot], axis = 1)\n",
        "                pred_rew = self.rew_model(torch.tensor(rew_model_input).to(self.device)) # Nawid -  I believe I do not need to rescale for a True of false situation\n",
        "                unroll_rewards += (1 - pred_rew.cpu().detach().numpy())\n",
        "            else:\n",
        "                #print('using oracle reward')\n",
        "                pred_rew = self.rew_model.predict_reward(m_obs, sampled_actions,next_obs)\n",
        "                unroll_rewards += pred_rew\n",
        "        \n",
        "            m_obs = next_obs # Nawid - Update the state after calculating the new state and calculating the rewards\n",
        "\n",
        "            if t ==0:\n",
        "                first_sampled_actions = sampled_actions\n",
        "        \n",
        "        self.env_model.train()\n",
        "        self.rew_model.train()\n",
        "\n",
        "        # Best the position of the sequence with the higher reward\n",
        "        arg_best_reward = np.argmax(unroll_rewards)\n",
        "        best_sum_reward = unroll_rewards[arg_best_reward].squeeze()\n",
        "        # take the first action of this sequence\n",
        "        best_action = first_sampled_actions[arg_best_reward]\n",
        "        #best_action =  np.squeeze(best_action)\n",
        "        return best_action, best_sum_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ahvG8lhSVp",
        "colab_type": "text"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3T5LHY6hRW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_data,val_data,model,batch_size, max_model_iter, optimizer, device,early_stopper,desired_model='Env_model'):\n",
        "    ''' \n",
        "    General function to train either of the two models\n",
        "    '''\n",
        "    # Unpack data\n",
        "    (X_train, y_train), (X_val, y_val) =  train_data, val_data\n",
        "    losses_env = []\n",
        "\n",
        "    # Choose loss function based on what type of model is training\n",
        "    if desired_model =='Env_model': # Nawid -  Decides which loss function to use\n",
        "        loss_function = model_MSEloss\n",
        "    else:\n",
        "        loss_function = model_BCEloss\n",
        "        #print('BCE being used')\n",
        "\n",
        "    # go through max_model iter supervised iterations\n",
        "    for it in tqdm(range(max_model_iter)):\n",
        "        # create mini batches of size batch_size\n",
        "        for mb in range(0,len(X_train), batch_size): # Nawid- Batch size is the step size\n",
        "            if len(X_train) > mb + BATCH_SIZE:\n",
        "                X_mb = X_train[mb:mb+BATCH_SIZE]\n",
        "                y_mb = y_train[mb:mb+BATCH_SIZE]\n",
        "                #X_mb += np.random.normal(loc = 0, scale = 0.001, size= X_mb.shape)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # forward pass of model to compute the output\n",
        "                pred_mb = model(torch.tensor(X_mb).to(device))\n",
        "                \n",
        "                # compute the loss\n",
        "                loss = loss_function(pred_mb,y_mb,device) #Nawid-  Uses Mse loss if dynamics model or uses BCE loss if reward/done model\n",
        "                wandb.log({'{} Training loss'.format(desired_model):loss.cpu().detach().numpy()})\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                # optimization step\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Nawid - Calculate the validation loss after each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_val = model(torch.tensor(X_val).to(device))\n",
        "            val_loss = loss_function(pred_val, y_val,device)\n",
        "            wandb.log({'{} Validation loss'.format(desired_model):val_loss})\n",
        "        \n",
        "\n",
        "        # Checks whether to early stop after each epoch\n",
        "        early_stopper(val_loss,model)\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dChd_BVNUKHR",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gdFb0bqHHN",
        "colab_type": "code",
        "outputId": "0ee47796-96b9-41c0-8101-5948bab3a4f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "wandb.init(entity=\"nerdk312\", project=\"ATARI_LABELS_MPC\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/2kt0mhkt\" target=\"_blank\">https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/2kt0mhkt</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/nerdk312/ATARI_LABELS_MPC/runs/2kt0mhkt"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9wLqaPmqqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ENV_NAME = 'MsPacmanNoFrameskip-v4'\n",
        "ENV_NAME = 'MsPacmanDeterministic-v4'\n",
        "feature_size = 10\n",
        "#feature_size = all_defaults['feature_size'] # Nawid- Dimensionality of the representation\n",
        "# workers = 8 # Nawid - Choosing the number of workers for the network\n",
        "\n",
        "# Main loop hyperp\n",
        "env_model_pretrain =  False\n",
        "rew_model_pretrain = False\n",
        "if env_model_pretrain:\n",
        "    pretrained_env_model = '/content/gdrive/My Drive/MsPacman-data/Env_model_4_9.47.41.pt'\n",
        "\n",
        "if rew_model_pretrain:\n",
        "    pretrained_rew_model = '/content/gdrive/My Drive/MsPacman-data/Rew_model_1000_randtraj_lr1e-4.pt'\n",
        "    AGGR_ITER = 10\n",
        "else:\n",
        "    AGGR_ITER = 100\n",
        "\n",
        "STEPS_PER_AGGR = 100\n",
        "\n",
        "# Random MB hyperp\n",
        "NUM_RAND_TRAJECTORIES = 1\n",
        "\n",
        "# 'cuda' or 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Supervised Model Hyperp\n",
        "ENV_LEARNING_RATE = 1e-3\n",
        "REW_LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1024\n",
        "TRAIN_ITER_MODEL =  100\n",
        "\n",
        "# Controller Hyperp\n",
        "HORIZON_LENGTH = 5\n",
        "NUM_ACTIONS_SEQUENCES = 200\n",
        "\n",
        "load_data = False\n",
        "if load_data:\n",
        "    loaded_trajectories = '/content/gdrive/My Drive/MsPacman-data/pacman_rand1000.npy'\n",
        "\n",
        "collect_data = False\n",
        "observation_channels = 1\n",
        "action_dim = 1\n",
        "n_actions = 5 #9 - Nawid - Change to 5 actions as the 4 other actions are simply copies of the other actions, therefore 5 actions should lower the amount of data needed.\n",
        "reward_dim = 1\n",
        "\n",
        "# Decides if an episode is done or not\n",
        "single_life = True\n",
        "\n",
        "# Time and date information\n",
        "now = datetime.datetime.now()\n",
        "date_time = \"{}_{}_{}-{}_{}\".format(now.day,now.month,now.year, now.hour, now.minute)\n",
        "\n",
        "# Making the network deterministic - https://pytorch.org/docs/stable/notes/randomness.html\n",
        "random_seed = 0\n",
        "normalise_on = False\n",
        "if random_seed:    \n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    #np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfc4B_LZUPki",
        "colab_type": "text"
      },
      "source": [
        "# Saving Config file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5P-E36KnFoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = wandb.config\n",
        "config.batch_size = BATCH_SIZE          \n",
        "config.horizon_length = HORIZON_LENGTH\n",
        "config.num_action_seq = NUM_ACTIONS_SEQUENCES\n",
        "config.train_model_iter = TRAIN_ITER_MODEL \n",
        "config.num_rand_trajectories = NUM_RAND_TRAJECTORIES\n",
        "config.aggr_iter = AGGR_ITER\n",
        "config.steps_per_aggr = STEPS_PER_AGGR\n",
        "config.env_lr = ENV_LEARNING_RATE\n",
        "config.rew_lr = REW_LEARNING_RATE\n",
        "config.no_actions = n_actions\n",
        "config.load_data = load_data\n",
        "config.collect_data = collect_data\n",
        "config.env_model_pretrain = env_model_pretrain\n",
        "config.rew_model_pretrain = rew_model_pretrain\n",
        "config.single_life = single_life\n",
        "config.random_seed = random_seed\n",
        "\n",
        "if load_data:\n",
        "    config.loaded_trajectories = loaded_trajectories\n",
        "\n",
        "if env_model_pretrain:\n",
        "    config.pretrained_env_model = pretrained_env_model\n",
        "if rew_model_pretrain:\n",
        "    config.pretrained_rew_model = pretrained_rew_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eIBf1pknbf_",
        "colab_type": "text"
      },
      "source": [
        "# Main - Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk1xCx6x7NZ2",
        "colab_type": "text"
      },
      "source": [
        "Data collection and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVutEvgUib8y",
        "colab_type": "code",
        "outputId": "1cbaeb01-4525-470e-d454-546ef887774c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# gather the dataset of random sequences\n",
        "data_collector = Data_collection(ENV_NAME,feature_size,normalise_on,n_actions)\n",
        "\n",
        "if load_data:\n",
        "    rand_dataset = np.load(loaded_trajectories,allow_pickle=True)\n",
        "else:\n",
        "    rand_dataset = data_collector.gather_random_trajectories(NUM_RAND_TRAJECTORIES)\n",
        "    filename = '/content/gdrive/My Drive/MsPacman-data/rand_traj_{}-'.format(NUM_RAND_TRAJECTORIES) + date_time\n",
        "    np.save(filename,rand_dataset)\n",
        "\n",
        "rl_dataset = []\n",
        "env_train_data, env_val_data, rew_train_data, rew_val_data = data_collector.collate_data(rand_dataset,rl_dataset)\n",
        "\n",
        "norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = data_collector.normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data)\n",
        "norm = (X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trajectory number : 0\n",
            "number random examples: 91 len(D_train_rand) 73 len(D_valid_rand) 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJi8pwC27Q5N",
        "colab_type": "text"
      },
      "source": [
        "Dynamics model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h62P5907GbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_model = NNDynamicsModel(n_actions + feature_size, feature_size).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "\n",
        "if env_model_pretrain:\n",
        "    env_model.load_state_dict(torch.load(pretrained_env_model))\n",
        "    env_model.eval()\n",
        "    \n",
        "else:\n",
        "    env_optimizer = torch.optim.Adam(env_model.parameters(),ENV_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "    wandb.watch(env_model, log=\"all\")\n",
        "    env_model_name = 'Env_model'+ '_' + date_time\n",
        "    early_stopping_env = EarlyStopping_loss(patience=5, verbose=True, wandb=wandb, name=env_model_name)\n",
        "    \n",
        "    for n_iter in range(AGGR_ITER):\n",
        "        if early_stopping_env.early_stop:\n",
        "            print('Early stopping')\n",
        "            break\n",
        "        train_model(norm_env_train_data, norm_env_val_data, env_model, BATCH_SIZE, TRAIN_ITER_MODEL,env_optimizer,device,early_stopping_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTOm9XOX7kOL",
        "colab_type": "text"
      },
      "source": [
        "Training the reward function and MPC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9DeBR47Lpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "rew_model = NNRewardModel(n_actions + feature_size, reward_dim).to(device) # Nawid - Need to put both actions as it is a one-hot vector\n",
        "\n",
        "if rew_model_pretrain:\n",
        "    rew_model.load_state_dict(torch.load(pretrained_rew_model))\n",
        "\n",
        "rew_optimizer = torch.optim.Adam(rew_model.parameters(),REW_LEARNING_RATE) # Nawid - Optimizer for the env model\n",
        "wandb.watch(rew_model, log=\"all\")\n",
        "Rew_model_name = 'Rew_model'+ '_' + date_time\n",
        "early_stopping_rew = EarlyStopping_loss(patience=5, verbose=True, wandb=wandb, name=Rew_model_name)\n",
        "\n",
        "\n",
        "Controller = multi_model_based_control(ENV_NAME,feature_size, normalise_on,n_actions,norm,env_model, rew_model, NUM_ACTIONS_SEQUENCES, HORIZON_LENGTH)\n",
        "global_step = 0\n",
        "for n_iter in range(AGGR_ITER):\n",
        "    if early_stopping_rew.early_stop:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "\n",
        "    if not rew_model_pretrain:\n",
        "        train_model(norm_rew_train_data, norm_rew_val_data, rew_model, BATCH_SIZE, TRAIN_ITER_MODEL,rew_optimizer,device,early_stopping_rew,desired_model='Rew_model')\n",
        "\n",
        "    \n",
        "    if collect_data:\n",
        "        env_train_data, env_val_data, rew_train_data, rew_val_data = data_collector.collate_data(rand_dataset,rl_dataset)\n",
        "        norm_env_train_data, norm_env_val_data, norm_rew_train_data, norm_rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler = normalise_dataset(env_train_data, env_val_data, rew_train_data, rew_val_data, X_env_state_scaler, y_env_scaler, X_rew_state_scaler)\n",
        "\n",
        "    obs = Controller.env.reset()\n",
        "    Controller.env.seed(0) # Set the random seed of the environment\n",
        "    #obs = env.reset()\n",
        "    #initial_info_labels = env.labels()\n",
        "\n",
        "    num_examples_added = 0\n",
        "    game_reward = 0\n",
        "    # records how long the agent survives for\n",
        "    timesteps = 0\n",
        "    controller_pred_rews = []\n",
        "    rews = []\n",
        "    # records how many simulations have occurred\n",
        "     \n",
        "    #i = 0 \n",
        "    while num_examples_added < STEPS_PER_AGGR:\n",
        "        while True:\n",
        "            tt = time.time()\n",
        "            \n",
        "            if Controller.repeated_initial: # Placed the repeated initial before the check_initial info labels since if the actions were chosen by MPC for the 264 intial actions, it would increase the computational time by quite a lot\n",
        "                #i += 1\n",
        "                #print(i)\n",
        "                obs,_,_,info = Controller.env.step(0) # Any action taken\n",
        "                info_labels = info['labels']\n",
        "                # Checks if the initial set is being repeated and if it isnt, it sets the value off\n",
        "                Controller.check_initial(info_labels) \n",
        "            else:\n",
        "                # new state achieved\n",
        "                state = Controller.state_conversion(obs,info_labels)\n",
        "                action, pred_rew = Controller.random_sampling_shooting(state)\n",
        "                action_one_hot = Controller.one_hot(action)\n",
        "                controller_pred_rews.append(pred_rew)\n",
        "\n",
        "                # one step in the environment with the action returned by\n",
        "                next_obs, reward,done, next_info = Controller.env.step(action)\n",
        "                next_info_labels = next_info['labels']\n",
        "                \n",
        "                next_state = Controller.state_conversion(next_obs, next_info_labels)\n",
        "                # Obtains the reward, done and sets whether repeated initial should be true or not\n",
        "                reward, pacman_done = Controller.reward_conversion(reward,done,info_labels, next_info_labels) \n",
        "\n",
        "                # add to the RL dataset                \n",
        "                rl_dataset.append([state, next_state, reward, pacman_done, action_one_hot])\n",
        "\n",
        "                num_examples_added += 1\n",
        "                timesteps +=1 \n",
        "                obs = next_obs\n",
        "                info_labels = next_info_labels\n",
        "\n",
        "                game_reward += reward \n",
        "                if done or (single_life and pacman_done):\n",
        "                    global_step += 1\n",
        "                    obs = Controller.env.reset()\n",
        "                    Controller.env.seed(0) # Need to set the random seed after the environment is done\n",
        "                    wandb.log({'game reward':game_reward, 'pred_rew': np.mean(controller_pred_rews),'survival time':timesteps,'global step':global_step })\n",
        "                    print('  >> R: {:.2f}, Mean sum:{:.2f},Survival time: {},Num examples:{}'.format(game_reward, np.mean(controller_pred_rews),timesteps,num_examples_added))\n",
        "                    rews.append(game_reward)\n",
        "                    game_reward = 0\n",
        "                    timesteps = 0\n",
        "                    controller_pred_rews = []\n",
        "                    break\n",
        "\n",
        "    print('  >> Mean: {:.2f}', np.mean(rews))    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}